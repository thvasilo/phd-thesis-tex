\section{Block-distributed Gradient Boosted Trees}
\label{sec:block-gbt}

In this section we will summarize our work on block-distributed
gradient boosted trees. We set up the motivation for the work
and related research in Section \ref{sec:block-gbt-background}.
we describe the algorithms we developed in Section \ref{sec:block-gbt-method},
and present our experimental evaluation in Section \ref{sec:block-gbt-results}.

\subsection{Background}
\label{sec:block-gbt-background}

Gradient Boosted Trees (GBT) have risen as one of the most popular algorithms for
tasks like Click-Through Rate prediction \cite{ctr-facebook}, learning-to-rank \cite{mcrank} and have been
successfully applied in multiple domains, becoming the dominant
algorithm used in data-mining competitions~\cite{xgboost}.
The success of GBTs has been driven to a large degree by their
accuracy and scalability: GBTs are fast to train and can handle massive datasets
through distributed implementations \cite{xgboost, lightgbm}, while being
highly accurate in variety of domains \cite{xgboost, hundreds-classifiers}.

The popularity of GBTs has created a large interest
in improving their scaling characteristics,
both in terms of efficient implementations \cite{xgboost, lightgbm, dimboost},
as well as specialized algorithms to improve their computation and communication
costs \cite{comm-efficient-gbt, online-gradient-boosting}.

All current distributed implementations of GBTs assume that the dataset is
\emph{horizontally} distributed, that is, each worker has access to
a subset of the data points with all their features included.
This work aims to provide a new way to scale-out GBTs by allowing the training
dataset to be distributed along \emph{both} the row and feature dimensions.

This new way of distributing the data introduces new algorithmic challenges,
as the points where the algorithm require communication increase, but also new opportunities
for optimization through the use of a more flexible communication pattern.
Specifically, while all existing methods use dense communication,
we use sparse communication achieving orders of magnitude reduced communication
cost for sparse data.

\subsection{Method}
\label{sec:block-gbt-method}




\subsection{Main Findings}
\label{sec:block-gbt-results}
