%!TEX root = ../main.tex

\begin{sammanfattning}

\begin{otherlanguage}{swedish}
	Algoritmer för maskininlärning används i allt högre grad i praktiskt taget alla delar av våra liv. En anledning till deras framgång är förmågan att lära sig komplexa representationer från enorma datamängder. Datorers beräkningshastighet ökar inte lika snabbt som de volymer av data vi vill lära oss från, vilket naturligt leder till algoritmer som måste vara resurseffektiva och parallella.
	I och med att användandet av maskininlärning fortsätter att breda ut sig blir förmågan att anpassa sig till en föränderlig värld och att hantera osäkerhet allt viktigare.

	I denna avhandling utvecklar vi skalbara algoritmer för maskininlärning med ett fokus på effektiva distribuerade online-beräkningar. Vi använder oss av \emph{approximationer} för att dramatiskt reducera beräkningskostnaden jämfört med exakta algoritmer, utvecklar algoritmer för \emph{inkrementell inlärning} som hanterar föränderliga miljöer med begränsad beräkningsbudget, samt utvecklar parallella och distribuerade algoritmer som garanterar att våra algoritmer kan hantera massiva datamängder.

	Avhandlingen börjar med att beskriva en skalbar algoritm för att beräkna likhet mellan noder i en graf och för att upptäcka begrepp. Vi visar dess användbarhet i flera olika domäner och dess skalbarhet genom att träna på en av de största tillgängliga textsamlingarna på ett fåtal minuter.
	Baserat på en praktisk tillämpning inom prediktion av längden på sessioner i media-strömning föreslår vi sedan ett flertal förbättringar på sätt att träna beslutsträd. Vi beskriver två algoritmer för att skatta osäkerheten för prediktioner gjorda av så kallade ``online random forests''. Vår metod uppvisar bättre träffsäkerhet än de bäst presterande metoderna inom forskningsfältet, samtidigt som den tar avsevärt mindre tid att köra.

	Avhandlingen föreslår sedan en parallell och distribuerad algoritm för så kallad ``online tree boosting'' som ger samma garantier gällande korrekthet som seriella algoritmer samtidigt som den i genomsnitt är flera storleksordningar snabbare.
	Slutligen föreslår vi en algoritm som tillåter gradientbaserad träning av beslutsträd att distribueras över både datapunkts- och attributs-dimensioner. Vi visar att vi kan reducera mängden kommunikation avsevärt för glesa datamängder i jämförelse med existerande metoder som bara distribuerar beräkningarna längs datapunktsdimensionen och använder täta representationer av datamängder.
\end{otherlanguage}

\end{sammanfattning}


