\chapter{Online Learning}
\label{ch:bg-online-learning}

In this chapter we will provide an introduction to online learning, which is the learning
setting for Papers \boostvhtNum and \uncertaintreesNum. We start by providing an brief
overview of the kinds of models available in Section \ref{sec:bg-ol-models}
and identify issues and solutions for the evaluation of online models in Section \ref{sec:bg-ol-evaluation}.
Since our work focuses on ensemble methods we present some popular online ensemble algorithms in Section
\ref{sec:bg-ol-ensembles} and finish the chapter with a look at the available
open-source software for online learning in Section \ref{sec:bg-ol-software}.

\section{Models}
\label{sec:bg-ol-models}

As with batch learning, there exists of variety of models in the online
learning domain. Commonly, these are models that were first developed
for the batch setting and subsequently adapted to online learning.
In this section we focus on supervised learning methods for
classification and regression, and explore different
learning representations to demonstrate the variety of
models available for online learning, following the
classification in \cite{onlineML}. For a comprehensive
introduction to the various models available in streaming
learning we refer the reader to \cite{moa-book}.
An important aspect of online learning that we do not cover in this chapter is learning
under concept drift, that is, when the underlying distribution
of the data changes during learning. We refer the reader
to the surveys \cite{concept-drift-survey-indre, concept-drift-survey-gama}
for in-depth looks at the topic.

\subsection*{Instance Based Learning}
One of the simplest and most intuitive methods in machine learning
is Instance Based Learning (IBL), which usually takes the form of
a nearest neighbor model.
In Instance Based Learning, we make predictions based on the instances themselves, unlike
model-based approaches that try to extract a model (function) from the
data. IBL exemplifies many characteristics
that make it a good candidate for online learning, and the
IBLStreams work of \citet{ibl-streams} demonstrates how these
can put to use to create an efficient online learning model.
IBL algorithms are inherently incremental, and can be faster to
update compared to a model-based approach, however suffer from two
distinct disadvantages: First, we need to store the data points themselves, which in a
massive streaming data scenario can be infeasible. To deal with this
issue IBLStreams will evict the oldest examples once an upper limit on
the number of data points stored is reached. Second, inference can be much slower as it requires
finding the k nearest neighbors to the incoming dataset, which even when
using efficient indexing structures (like Locality Sensitive Hashing\cite{lsh-indyk, lsh-gionis}) can be very costly
compared to a model-based prediction, e.g. from a linear model. The
authors suggest that IBL makes more sense in scenarios where updates are
often but queries infrequent.

In terms of drift adaptation, IBLs can have
the advantage that removing the effect of older data points is easy
compared to say an ANN model, but it is still highly model-dependent.
The authors suggest different ways to deal with
concept drift in IBLStreams. They mention the limitations of a non-adaptive, window-based
approach and suggest that IBL methods are good candidates for learning in
under drift because of their locality property, that is, that introducing
a new example will only affect the predictions made around that region.
In theory, an example should only be included in the model if it improves the
accuracy, but that is impossible to know ahead of time. However, we can use heuristics
to determine when to discard examples. One could use the temporal and/or spatial
relevance, or consistency of the examples to the current concept.
In IBLStreams, the contents and size of the ``case base'',
i.e. the examples that form the nearest neighbor representation, is
updated automatically. IBLStreams tries to maintain a balance between
maintaining few examples in the case base in order to be able to deal
easier with concept drift, while keeping the set large enough so that
predictions under a single concept are accurate. To that end, they
use statistical change detection mechanisms and also provide ways
to update the parameters of the algorithm, e.g. the number of nearest
neighbors to examine, based on the observed error.

\subsection*{Linear Models}
In terms of model-based approaches, linear models are popular
for online learning due to their simplicity, efficiency and interpretability.
For regression these models have the general form~\cite{esl}:

\begin{equation}
	f(X) = \beta_0 + \sum_{j=1}^{p}{X_j\beta_j}
\end{equation}

\noindent
where $p$ the number of features in the dataset, where we assume a linear relationship between the dependent and independent predictors.

One of the earliest examples of an online linear classification model is the seminal
Perceptron algorithm \cite{perceptron} that tries to find a separating hyperplane
between two classes based on the distance of the misclassified points to the
decision boundary~\cite{esl}.
In the online setting, linear models are typically trained using a form
of stochastic gradient descent, that moves the coefficients
of the model according to the gradient of a single data point
and a step size parameter.
The Perceptron is a simple algorithm that however has many drawbacks
noted in \cite{esl}, regarding the stability of the algorithm
for perfectly separable data, its dependence on correctly setting
the step size, and the fact that the algorithm does not
converge when the data are not separable.
Another important family of linear algorithms designed for the online domain
are Passive-Aggressive (PA) algorithms \cite{passive-aggressive}.
These are margin-based algorithms for classification, regression and
sequence prediction that solve a constraint optimization problem.
For every incoming incoming example, PA algorithms
try to achieve a margin between the predicted value and true label
by either not changing the model (passive) when the margin is satisfied,
or by applying the necessary correction to the model coefficients
to enforce a zero loss when the margin is not satisfied (aggressive).
While being a linear model, PA algorithms can make use of the kernel trick to
employ non linear predictors, as done by the seminal work
on Support Vector Machines (SVM) by \citet{svm-book}, which we
focus on in the sequel.

\subsection*{Support Vector Machines}
Support Vector Machines, similar to the Perceptron, are algorithms for determining
optimal separating hyperplanes, but unlike the Perceptron, they can deal with
cases where the data are not linearly separable. They achieve that by using the
``kernel trick'' to transform the data and determining a linear boundary in
the transformed space~\cite{esl}.
SVMs have also
been extended to work in the online domain \cite{online-kernels}
where the main challenge is the need to maintain a set of support vectors
in memory, as they grow linearly with the number of prediction errors
\cite{budget-classification}. The work of \citet{budget-classification}
deals with this issue by providing an online learning algorithm
that enforces sparsity through an insertion and deletion phase.
Once an erroneous example is inserted, the algorithm will look
for past examples that are made redundant by the new point,
and remove those to save memory.
Another approach is taken by the Forgetron \cite{forgetron} which,
for every mistake made by the algorithm, runs the standard
Perceptron update, shrinks the support vector coefficients, and removes
the support vectors with the smallest coefficients.
The Passive-Aggressive algorithm has also been modified
to perform kernel learning on a budget \cite{pa-budget},
i.e. with a bounded model size, by introducing an additional
constraint to the original problem that removes one support
vector for every new error once the budget is met.
One key drawback of these methods
is the need to determine a budget for the model size beforehand.
More recently, new methods have been proposed to scale up
online kernel learning to massive datasets while maintaining a bounded model size,
by transforming the feature space, and then performing linear learning \cite{large-online-kernels}
or by approximating new instances by ``core points''
scattered across the input domain~\cite{approximation-vm}.

\subsection*{Rule-based Learning}
Decision rules \cite{decision-lists} and decision trees \cite{breiman1984cart} are two closely related learning methods
that make predictions in terms of a set of \emph{if-then} rules. Since
decision trees play a central role in our research we dedicate a separate
chapter to review them (Chapter \ref{ch:bg-decision-trees}) and focus on
online decision rules here. Decision rules use conjunctions of conditions
on the attribute values to make predictions of the form
\texttt{if <conditions> then <prediction>}. This structure makes them
some of the more easily interpretable models available.
In the classification task
these are typically learned by maximizing the information gain
of introducing a rule for a given outcome.
\citet{decision-rules-streams}
provided one of the first decision list learning algorithms aimed at the streaming
domain, which updates its rules by maintaining an up-to-date set of sufficient statistics for each
rule. Rules are expanded by selecting the condition, for example a threshold on
a numerical feature, that minimizes the entropy of the class labels of the example
that are covered by that rule~\cite{decision-rules-streams}.
The method uses the Hoeffding bound \cite{hoeffding-bound} to determine when it is time
to update a rule, either by expanding an existing one or introducing a new rule.
The process was later expanded to handle cases of concept drift \cite{adaptive-rules-classification}
and regression~\cite{adaptive-rules-regression}.


% TODO
%\subsection*{Online Gaussian Processes}
%
%Gaussian Processes (GP) can be viewed as a Bayesian alternative to kernel methods
%like SVM~\cite{ml-book-murhpy}. As defined by \citet{ml-book-murphy}, GPs work by inferring a distribution over
%functions given the data, assuming a jointly Gaussian distribution of
%the function values, with the covariance matrix given a kernel value
%over all pairs of data points. One distinct advantage GPs have over
%SVMs is the ability to quantify the uncertainty in their predictions,
%as GPs are probabilistic models.
%However,Gaussian Processes
%have a time complexity of $O(N^3)$ limiting the base GP algorithm to small
%data scenarios. To address this shortcoming, there have been
%multiple attempts to improve their computational characteristics, and adapt them
%to the online learning domain, see \cite{gp-review} for a recent survey on scalable
%GPs.
%
%One approach for scalable GPs is the use of \emph{inducing points} in order to
%introduce sparsity in the GP and take advantage of matrix structures to achieve
%faster computation. The online sparse GPs proposed by \citet{online-sparse-gp}
%work by keeping an up-to-date Cholesky factor of the sparse covariance matrix,
%instead of updating the covariance matrix itself. For kernel functions with
%local support this leads to linear time online updates.




%\section{Drift Adaptation}
%\label{sec:bg-ol-adapation}

\section{Evaluation}
\label{sec:bg-ol-evaluation}

The evaluation of online learning algorithms differs from the batch
case, not only because of the potential for a concept drift occurring,
but also due to considerations about the computational cost of the
methods themselves. The research on valid evaluation methods for
streaming methods is somewhat limited however, with the notable exception
of the work by \citet{online-evaluation-kdd}, further expanded in
\cite{online-evaluation-journal}\todo{Maybe mention "The comparison and Evaluation of Forecasters"}.
In this section we provide
a brief overview of the challenges and proposed solutions for
the evaluation of online learning algorithms and refer the
reader to \cite{online-evaluation-journal} for an in-depth look
into these issues.

\citet{online-evaluation-journal} identify three areas that
become important in the evaluation of streaming learning
algorithms: \emph{Space} in terms of the memory requirement of the algorithm, \emph{learning time}, and \emph{generalization power}.
Each of these aspects is important in the online learning scenario, and
we will discuss here the space and generalization power aspects, since
learning time is a more intuitive concept.


Online learning algorithms are commonly designed for limited resource environments,
and as such, the space requirements of the algorithm are an important consideration.
Apart from the theoretical analysis of the space cost of the algorithms,
their empirical costs should also be analyzed, as the empirical differences
between algorithms with the same theoretical memory cost can be
significant. \citet{ram-hours} introduced
the concept of \emph{RAM-hours} for this purpose, with the aim of estimating
the cost-efficiency of an algorithm in a cloud environment where instances
are charged according to the time taken to use them, and instances with
higher memory capacities are usually more expensive.
RAM-hours measure the empirical memory consumption of an
algorithm, with one GB of RAM deployed for an hour equaling one RAM-hour.
\citet{vfdt}
make specific mention of the memory requirements of the online decision
tree algorithm they develop, and provide solutions that make the algorithm
memory bounded. However as \citet{online-evaluation-journal} mention, this
is an aspect often overlooked in the evaluation of online learning
algorithms.

In terms of evaluating the generalization power of online learning
algorithms, one aspect that requires special attention is the fact
that the learner evolves as we train it with more samples, so its
performance in the early stages of learning can be very different
to later stages. The two strategies proposed as viable evaluation strategies for the online
setting in \cite{online-evaluation-journal} are \emph{holdout testing}
and \emph{prequential evaluation}. Holdout testing refers to the
established evaluation method from batch learning, where we train
our algorithm on a subset of the data, the \emph{training set},
and evaluate its performance on a set of unseen data, the
\emph{test set} or \emph{holdout set}. This requires us to set a
specific interval at which we check the performance of the algorithm
on the test set, although theoretically we could also perform this
test after each example, introducing however a large computational overhead if
the test set is large.

An alternative is to use
predictive sequential (prequential) evaluation where we present
an example to the algorithm, make a prediction, update our metric,
and finally reveal the example's label and proceed to train the
algorithm with it. This method can also be used in situations
where the label is available for only few of the points in the
data set. However, because the model will exhibit worse performance
in the early learning stages, it is recommended to apply a forgetting
factor to the metric, so that large errors made in the start do
not severely affect the overall evaluation of the algorithm. This
can be done using sliding windows, i.e. evaluating the error using
overlapping subsets of the incoming dataset, or using fading
factors which discount the error for older examples \cite{online-evaluation-kdd}.
These have the added advantage of being faster to compute and memory-less
compared to using sliding windows.

One common issue in classification, in the batch as well as the online setting, is class
imbalance.
For example, on a dataset where 90\% of the instances belong to a single
class, a simple majority classifier will achieve 90\% accuracy, although no
real learning is taking place.
To deal with this problem in the online setting, \citet{kappa-statistic}
proposed the Kappa statistic, and its extension $\kappa_m$ \cite{kappa-m},which is a robust estimator that takes into consideration
the probability that a classifier will produce the correct prediction by chance.
The metric has also been expanded to deal with temporal dependence
in the labels of subsequent examples \cite{temporal-dependence}.
More recently, \citet{prequential-auc} proposed an online adaptation
of the area under the ROC curve metric (AUC) \cite{auc}, called
\emph{prequential AUC} which can deal with the class-imbalance
problem also in the presence of concept drift, and performed
an evaluation against metrics like the Kappa statistic to show
that each metric captures a different aspect of the algorithm's performance.

\section{Online Ensemble Methods}
\label{sec:bg-ol-ensembles}

Due to their approximate nature, online learning models often demonstrate
limited accuracy compared to their batch counterparts. Ensemble methods
have been shown to vastly improve the bias \& variance characteristics
of a wide range of models \cite{ensemble-methods-dietrich} and have therefore
also been a focus in the online learning literature.
In this section we will provide an introduction to some of the more established
online ensemble methods that we have also used in our work, along with related
recent work. We refer the interested to the survey by \citet{online-ensembles-survey}
for a more in-depth look at online ensemble methods.

The two main paradigms in ensemble algorithms are bagging \cite{bagging} and boosting \cite{boosting-freund, boosting-schapire}. In this section we will review how each of these algorithms
has been adapted to the online setting. We will focus our descriptions on the first, and more established,
work that deals with both online bagging and boosting proposed by \citet{Oza2001online}.

\subsection{Online Bagging}

Briefly, bagging works by training an ensemble of learners in parallel,
each trained on a bootstrap sample~\cite{bootstrap} of the original dataset.
The predictions of each learner are averaged to produce the final outcome.

The online bagging model proposed by \citet{Oza2001online} and further explored in
\cite{online-bagging-experiments} is an intuitive algorithm, listed in Algorithm
\ref{alg:ozabag}. We make use of this algorithm in \uncertaintrees.

\begin{algorithm}
	\small
	\caption{OzaBag(\ensemble, $L_o$, $(x,y)$)}
	\label{alg:ozabag}
	\Initialization{$\lambda^c_t \define \lambda^w_t \define 0 , \quad \forall t \in [1,s]$}
	\Input{\ensemble, the ensemble, a set of $s$ hypotheses $h_t$;
		$L_o$, an online learning algorithm;
		$(x,y)$, a labeled training instance.}
	\Output{hypotheses $h_t$ trained on $(x,y)$.}

	\ForEach(\tcp*[f]{in order $t \in [1,s]$}){$h_t \in \ensemble$}{
		$k$ \define Poisson($\lambda = 1$) \;
		Assign weight $k$ to $(x,y)$ \;
		$h_t \define L_o(h_t, (x,y))$
	}
	\Return $\widehat{y}$ \;
\end{algorithm}

The main insight of the algorithm, commonly referred to as \emph{OzaBag}, is that we can approximate the binomial distribution
used to determine whether a sample will be included the bootstrap
sample using a Poisson distribution\todo{Verify this claim, is it in the orig. paper?}. The distribution of bagging sample sizes, $K$, tends
to a Poisson($\lambda = 1$) as the number of samples $N \rightarrow \infty$. To use bagging online then,
for each incoming example and
for each member of the ensemble \ensemble, we draw a sample from a Poisson($\lambda = 1$). We use to the drawn scalar $k$ to modify the weight of the incoming instance, and train the algorithm using the updated weight. \citet{Oza2001online} prove that as the number of samples N grows
to infinity, the distribution of the online training set will converge to that of the batch algorithm.

This algorithm has been adapted and extended in multiple online bagging methods. \citet{online-bag-imbalance}
make use of this strategy to deal with the class imbalance problem in online manner. Their strategy
is to adjust the $\lambda$ parameter of the Poisson distribution so that data points with underrepresented
classes have a larger effect on the training. \citet{new-ensemble-methods} provide two alternative
bagging methods with the purpose of dealing with concept drift, one that uses trees of different
sizes, and one that makes use of the ADWIN \cite{adwin} change detection method on top Oza's bagging algorithm to detect when a concept drift has occurred. Leveraging Bagging \cite{leveraging-bagging} is an improvement to the previous model,
where the authors increase the $\lambda$ parameter of the Poisson distribution to ``increase the diversity of the weights`` and use the method of \citet{multiclass-codes} to handle multi-class cases
as binary classification using error correcting codes.

\subsection{Online Boosting}

Boosting works by combining the predictions of many weak learners to produce a
strong learner. The process adjusts the weight distribution of the data points
at each iteration, assigning more weight to ``difficult'' instances that the
current ensemble mis-predicts, forcing subsequent iterations to focus on those
examples. The difficulty in online boosting is that
instances arrive sequentially, making it challenging to maintain a distribution
of weights.

Online boosting has found more widespread use compared to online bagging ensembles,
as it's been used successfully in many computer vision applications, with a focus on
object tracking \cite{online-boost-cv4, online-boost-cv, online-boost-cv3, online-boost-cv2, online-boost-cv5, online-boost-cv-6}.
As a result many different boosting algorithms have been proposed, some aimed at a specific application like object tracking, and others that are general learning algorithms.

We will focus on the original algorithm proposed by \citeauthor{Oza2001online},
which we also make use of in \boostvht,
and the more recent general online boosting algorithms that improve upon the
theoretical guarantees and performance of the original.

The original online boosting algorithm by \citeauthor{Oza2001online}, referred to as
OzaBoost, is listed
in Algorithm \ref{alg:ozaboost}.
The learning process is similar to that of OzaBag (Algorithm \ref{alg:ozabag}), but now the
algorithm is strictly sequential, and the weight the example takes depends on whether
the previous member of the ensemble was able to correctly classify the example.
Specifically, the algorithm tries to assign half the total weight to the misclassified
examples on the next stage, and the other half to the correctly classified ones.
This is done by keeping track of the $\lambda$ parameter sums for the two cases,
correct and incorrect classification. We update the weight of an example
before it passes on to the next member of the ensemble accordingly, increasing
the weight every time is incorrectly classified, and decreasing it every time it
is correctly classified.  One thing to note about OzaBoost is that it requires
that we set the number of boosting rounds from the beginning, unlike the
batch AdaBoost algorithm.

Despite its popularity, OzaBoost lacks rigorous theoretical guarantees.
The first attempt to formalize online boosting was made by \citet{online-boosting-theoretical}.
They re-visit the assumption made about the performance of the weak
learners, namely that any weak learner will be able to
do better than random guessing, as it is not a realistic assumption
for the online setting where learner accuracy is more limited. They also
provide a way to not have to set the number of learners in the ensemble beforehand,
by dynamically assigning voting weights to learners.
However, doing so requires the setting of another parameter $\gamma$,
for which it is hard to determine good values.
Their algorithm, OSBoost,
extends the batch SmoothBoost \cite{smoothboost} algorithm, which
was designed as a boosting algorithm robust to noise, to the online
setting. They use the weighting scheme from that algorithm to assign
larger weights to incorrectly predicted examples, and prove that
their ensemble can use the set of weak learners to achieve a small
error.

The work of \citet{Beygelzimer2015optimal} improves upon OSBoost,
by providing an optimal algorithm in terms of error rate, and also provides
a parameter-free algorithm that does away with the need to set the $\gamma$
parameter of the OSBoost.
The optimal algorithm, Online.BBM, is based on the Boost-by-majority batch
algorithm \cite{batch-bbm} and relaxes the assumptions made by OSBoost,
while the parameter-free algorithm, AdaBoost.OL, makes use of online
loss minimization where the authors choose to minimize logistic loss
in order to avoid large weights which could adversely affect the
error rate of the algorithm.
We should note that all the above algorithms are strictly sequential
and are therefore hard to parallelize.
Our work in \boostvht is able to
make use of any online boosting algorithm to perform
parallel online boosting, while maintaining their guarantees.


\begin{algorithm}
	\small
	\caption{OzaBoost(\ensemble, $L_o$, $(x,y)$)}
	\label{alg:ozaboost}
	\Initialization{$\lambda^c_t \define \lambda^w_t \define 0 , \quad \forall t \in [1,s]$
		\tcp*[f]{cumulative weight of instances with correct and wrong predictions}}
	\Input{\ensemble, the ensemble, a set of $s$ hypotheses $h_t$;
		$L_o$, an online learning algorithm;
		$(x,y)$, a labeled training instance.}
	\Output{prediction $\widehat{y}$.}
	\tcp*[h]{prequential evaluation: first test...} \;
	$\widehat{y} = \argmax_{\dot{y} \in Y} \sum_{t = 1}^{s} \log \left( \frac{1-\epsilon_t}{\epsilon_t} \right) I\left( h_t(x) = \dot{y} \right)$ \;
	\tcp*[h]{...then train} \;
	$\lambda \define 1$\;

	\ForEach(\tcp*[f]{in order $t \in [1,s]$}){$h_t \in \ensemble$}{
		k \define Poisson($\lambda$) \;
		\While(\tcp*[f]{give weight $k$ to the instance}){k > 0}{
			$h_t \define L_o(h_t, (x,y))$ \;
			$k \define k-1$ \;
		}
		\If(\tcp*[f]{correct prediction}){$y = h_t(x)$}{
			$\lambda^c_t \define \lambda^c_t + \lambda$ \;
			$\epsilon_t \define \frac{\lambda^w_t}{\lambda^c_t + \lambda^w_t}$ \;
			$\lambda \define \lambda \left( \frac{1}{2(1 - \epsilon_t)} \right)$ \;
		}\Else(\tcp*[f]{wrong prediction}){
			$\lambda^w_t \define \lambda^w_t + \lambda$ \;
			$\epsilon_t \define \frac{\lambda^w_t}{\lambda^c_t + \lambda^w_t}$ \;
			$\lambda \define \lambda \left( \frac{1}{2 \epsilon_t} \right)$ \;
		}
	}
	\Return $\widehat{y}$ \;
\end{algorithm}

\section{Software}
\label{sec:bg-ol-software}

Compared to the multitude of options available for batch learning, the availability
of open-source software for online learning is relatively limited. In this section we
mention the most popular libraries for online learning, and point out the ones we have have used
and extended as part of this dissertation.

Perhaps the first online learning library released was the Very Fast Machine Learning (VFML) framework which was developed by \citet{vfdt} as part of their work on the Very Fast Decision Tree (VFDT) algorithm (which we describe in Chapter \ref{sec:bg-dt-online-trees}).
It includes the VFDT and its variations, as well as data pre-processing tools.

One of the most established open-source frameworks for online learning is MOA, which stands
for Massive Online Analysis \cite{bifet2010moa}. MOA includes a collection of
learning algorithms, evaluation methods and metrics, data generators as well as
a graphical interface to perform repeatable experiments. MOA is designed in the
vein of WEKA \cite{weka} and includes interfaces that allow inter-operability
between the algorithms available in WEKA and MOA. Like WEKA, MOA has an extensible
design that allows developers to re-use parts of the existing algorithms to develop
new methods, and easily evaluate new methods using the evaluation strategies mentioned
in Section \ref{sec:bg-ol-evaluation}, by adhering to a simple API. In our work,
we have used MOA to implement the algorithms of \uncertaintrees and as a baseline comparison
in \boostvht. Other single-worker online learning libraries
include LIBOL~\cite{libol} and more recently scikit-multiflow~\cite{sk-multiflow}.

One drawback of these libraries is that they are designed to run on a single machine and therefore
can have issues with massive, distributed streams. Apache SAMOA \cite{samoa} is an effort
to bring the design principles of MOA into the distributed setting, utilizing a platform-independent
design that allows it to run on top of many distributed stream processing engines like
Apache Flink \cite{flink}, Apache Storm~\cite{storm} and Apache Samza~\cite{samza}.
Similarly to MOA, it provides learning algorithms and evaluation methods, and its
design makes it easy to extend with new algorithms.
We have used SAMOA as the development framework for \boostvht.

Another library aimed at distributed stream learning is StreamDM \cite{streamdm}.
StreamDM also inherits some of the design aspects of MOA, providing similar interfaces
to access learning algorithms and run evaluations, but unlike SAMOA
is built to run on top of only the Apache Spark~\cite{spark} streaming engine.
While running on top of a distributed stream engine can make the development
of distributed algorithms easier, it can have a negative effect on the
performance of the library, due to the overhead introduced by the
engine. To tackle this is issue, an optimized version of StreamDM
was proposed in \cite{streamdmPP}. An earlier attempt at developing
a distributed streaming learning framework was Jubatus~\cite{jubatus}.

Finally, one of the most successful frameworks that relies on online learning,
albeit to deal with batch problems, is Vowpal Wabbit (VW) \cite{vw}. VW makes
use of parallel and distributed Stochastic Gradient Descent \cite{sgd} as its main
learning algorithm and has been extended to provide a number of online learning models,
including online boosting \cite{Beygelzimer2015optimal}, online Latent Dirichlet Allocation
\cite{ldaOnline}, and contextual bandits \cite{onlineBandits}.
