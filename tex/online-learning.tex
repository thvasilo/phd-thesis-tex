\chapter{Online Learning}
\label{ch:bg-online-learning}

In this chapter we will provide an introduction to online learning, which is the learning
setting for Papers \boostvhtNum and \uncertaintreesNum. We start by providing an brief
overview of the kinds of models available in Section \ref{sec:bg-ol-models}
and identify issues and solutions for evaluation in Section \ref{sec:bg-ol-evaluation}.
Since our work focuses on ensemble methods we present the online ensemble algorithms in Section
\ref{sec:bg-ol-ensembles} and finish the chapter with a look at the available
open-source software for online learning in Section \ref{sec:bg-ol-software}.

\section{Models}
\label{sec:bg-ol-models}

As with batch learning, there exists of variety of models in the online
learning domain. Usually these are models that were first developed
for the batch setting and subsequently adapted to online learning.
In this section we focus on supervised learning methods for
classification and regression, and explore different
learning representations to demonstrate the variety of
models available for online learning, following the
classification in \cite{onlineML}. For a comprehensive
introduction to the various models available in streaming
learning we refer the reader to \cite{moa-book}.
An important aspect of online learning that we do not cover in this chapter is learning
under concept drift, that is, when the underlying distribution
of the data changes during learning. We refer the reader
to the surveys \cite{concept-drift-survey-indre, concept-drift-survey-gama}
for in-depth looks at the topic.

\subsection*{Instance Based Learning}
One of the simplest and most intuitive methods in machine learning
is Instance Based Learning (IBL), which usually takes the form of
a nearest neighbor model. IBL exemplifies many characteristics
that make it a good candidate for online learning, and the
IBLStreams work of \citet{ibl-streams} demonstrates how these
can put to use to create an efficient online learning model.
In Instance Based Learning, we make predictions based on the instances themselves, unlike
model-based approaches that try to extract a model (function) from the
data. IBL algorithms are inherently incremental, and can be faster to
update compared to a model-based approach, however suffer from two
distinct disadvantages: First, we need to store the data which in a
massive streaming data scenario can be infeasible. To deal with this
issue IBLStreams will evict the oldest examples once an upper limit on
the size is hit. Second, inference can be much slower as it requires
finding the k nearest neighbors to the incoming dataset, which even when
using efficient indexing structures (like Locality Sensitive Hashing) can be very costly
compared to a model-based prediction, e.g. from a linear model. The
authors suggest that IBL makes more sense in scenarios where updates are
often but queries infrequent. In terms of drift adaptation IBLs can have
the advantage that removing the effect of older data points is easy
compared to say an ANN model, but it is still highly model-dependent.

In terms of the learning process of IBLStreams, the authors suggest ways to deal with
the concept drift scenario. They mention the limitations of a non-adaptive, window-based
approach and suggest that IBL methods are good candidates for learning in
under drift because of their locality property, that is, that introducing
a new example will only affect the predictions made around that region.
Since an example should only be included in the model if it improves the
accuracy, but that impossible to know ahead of time, we can use heuristics
to determine when to discard examples. One could use the temporal and/or spatial
relevance, or consistency of the examples to the current concept.
In the IBLStreams algorithm, the contents and size of the ``case base'',
i.e. the examples that form the nearest neighbor representation, is
updated automatically. IBLSTreams tries to maintain a balance between
maintaining few examples in the case base in order to be able to deal
easier with concept drift, while keeping the set large enough so that
predictions under a single concept are accurate. To that end, they
use statistical change detection mechanisms and also provide ways
to update the parameters of the algorithm, e.g. the number of nearest
neighbors to examine based on the observed error.

\subsection*{Linear Models}
In terms of model-based approaches, linear models are popular
for online learning due to their simplicity, efficiency and interpretability.
For regression these models have the general form~\cite{esl}:

\begin{equation}
	f(X) = \beta_0 + \sum_{j=1}^{p}{X_j\beta_j}
\end{equation}

and assume a linear relationship between the dependent and independent predictors.
One of the earliest examples of an online linear classification model is the seminal
Perceptron algorithm \cite{perceptron} that tries to find a separating hyperplane
between two classes based on the distance of the misclassified points to the
decision boundary~\cite{esl}.
In the online setting, linear models are typically trained using a form
of stochastic gradient descent, that moves the coefficients
of the model according to the gradient of a single data point
and a step size parameter.
This is a simple algorithm that however has many drawbacks
noted in \cite{esl}, regarding the stability of the algorithm
for perfectly separable data, its dependence on correctly setting
the step size, and the fact that the algorithm does not
converge when the data are not separable.
Another important family of linear algorithms designed for the online domain
are Passive-Aggressive algorithms \cite{passive-aggressive}.
These are margin-based algorithms for classification, regression and
sequence prediction that solve a constraint optimization problem.
For every incoming incoming example, PA algorithms
try to achieve a margin between the predicted value and true label
by either not changing the model (passive) when the margin is satisfied,
or by applying the necessary correction to the model coefficients
to enforce a zero loss when the margin is not satisfied (aggressive).
While being a linear model, PA algorithms can make use of the kernel trick to
employ non linear predictors, as done by the seminal work
on Support Vector Machines (SVM) by \citet{svm-book}.

\subsection*{Support Vector Machines}
SVMs have also
been extended to work in the online domain \cite{online-kernels}
where the main challenge is the need to maintain a set of support vectors
in memory, as they grow linearly with the number of prediction errors
\cite{budget-classification}. The work of \citet{budget-classification}
deals with this issue by providing an online learning algorithm
that enforces sparsity through an insertion and deletion phase.
Once an erroneous example is inserted, the algorithm will look
for past examples that are made redundant by the new point,
and remove those to save memory.
Another approach is taken by the Forgetron \cite{forgetron} which,
for every mistake made by the algorithm, runs the standard
Perceptron update, shrinks the support vector coefficients, and removes
the support vectors with the smallest coefficients.
The Passive-Aggressive algorithm has also been modified
to perform kernel learning on a budget \cite{pa-budget},
i.e. with a bounded model size, by introducing a additional
constraint to the original problem that removes one support
vector for every new error once the budget is met.
One key drawback of these methods
is the need to determine a budget for the model size beforehand.
More recently, new methods have been proposed to scale up
online kernel learning to massive datasets while maintaining a bounded model size,
by transforming the feature space, and then performing linear learning \citet{large-online-kernels}
or by approximating new instances by ``core points''
scattered across the input domain~\citet{approximation-vm}.

\subsection*{Rule-based Learning}
Decision rules \cite{decision-lists} and decision trees \cite{breiman1984cart} are two closely related learning methods
that make predictions in terms of a set of \emph{if-then} rules. Since
decision trees play a central role in our research we dedicate a separate
chapter to review them (Chapter \ref{ch:bg-decision-trees}) and focus on
online decision rules here. Decision rules use conjunctions of conditions
on the attribute values to make predictions of the form
\texttt{if <conditions> then <prediction>}. This structure makes them
some of the more easily interpretable models available.
In the classification task
these are typically learned by maximizing the information gain
of introducing a rule for a given outcome.
\citet{decision-rules-streams}
provided one of the first decision list learning algorithms aimed at the streaming
domain, which updates rules by updating a set of sufficient statistics for each
rule. Rules are expanded by selecting the condition, for example a threshold on
a numerical feature, that minimizes the entropy of the class labels of the example
that are covered by that rule~\cite{decision-rules-streams}.
The method uses the Hoeffding bound \cite{hoeffding-bound} to determine when it is time
to update a rule, either by expanding an existing one or introducing a new rule.
The process was later expanded to handle cases of concept drift \cite{adaptive-rules-classification}
and the regression task~\cite{adaptive-rules-regression}.



%[Online Gaussian Processes] \\



%\section{Drift Adaptation}
%\label{sec:bg-ol-adapation}

\section{Evaluation}
\label{sec:bg-ol-evaluation}

The evaluation of online learning algorithms differs from the batch
case, not only because of the potential for a concept drift occurring,
but also due to considerations about the computational cost of the
methods themselves. The research on valid evaluation methods for
streaming methods is somewhat limited however, with the notable exception
of the work by \citet{online-evaluation-kdd}, further expanded in
\cite{online-evaluation-journal}\todo{Maybe mention "The comparison and Evaluation of Forecasters"}.
In this section we provide
a brief overview of the challenges and proposed solutions for
the evaluation of online learning algorithms and refer the
reader to \cite{online-evaluation-journal} for an in-depth look
into these issues.

The authors identify three areas that
become important in the evaluation of streaming learning
algorithms: \emph{Space} in terms of the memory requirement of the algorithm, \emph{learning time}, and \emph{generalization power}.
Each of these aspects is important in the online learning scenario, and
we will discuss the space and generalization power aspects.


Since these algorithms are designed for limited resource environments,
the space requirements of the algorithm are an important consideration.
Apart from the theoretical analysis of the space cost of the algorithms,
their empirical costs should also be analyzed. \citet{ram-hours} introduced
the concept of \emph{RAM-hours} for this purpose, with the aim of estimating
the cost-efficiency of an algorithm in a cloud environment where instances
are charged according to the time taken to use them, and instances with
higher memory capacities are usually more expensive. \citet{vfdt} also
make specific mention of the memory requirements of the online decision
tree algorithm they develop, and provide solutions that make the algorithm
memory bounded. However as \citet{online-evaluation-journal} mention, this
is an aspect often overlooked in the evaluation of online learning
algorithms.

In terms of evaluating the generalization power of online learning
algorithms, one aspect that requires special attention is the fact
that the learner evolves as we train it with more samples, so it
performance in the early stages of learning can be very different
to later stages. The two strategies proposed as viable for the online
setting in \cite{online-evaluation-journal} are \emph{holdout testing}
and \emph{prequential evaluation}. Holdout testing refers to the
established evaluation method from batch learning, where we train
our algorithm on a subset of the data, the \emph{training set},
and evaluate its performance on a set of unseen data, the
\emph{test set} or \emph{holdout set}. This requires us to set
specific interval at which we check the performance of the algorithm
on the test set, although theoretically we could also perform this
test after each example, but for a large computational cost if
the test set is large.

An alternative is to use
predictive sequential (prequential) evaluation where we present
an example to the algorithm, make a prediction, update our metric,
and finally reveal the example's label and proceed to train the
algorithm with it. This method can also be used in situation
where the label is available for only few of the points in the
data set. However, because the model will exhibit worse performance
in the early learning stages, it is recommended to apply a forgetting
factor to the metric, so that large errors made in the start do
not severely affect the over evaluation of the algorithm. This
can be done using sliding windows, i.e. evaluating the error using
overlapping subsets of incoming dataset, or a using fading
factors which discount the error for older examples \cite{online-evaluation-kdd}.
These have the added advantage of being faster and memory-less
compared to using sliding windows.

One common issue in classification, in the batch as well as the online setting, is class
imbalance. To deal with this problem in the online setting, \citet{kappa-statistic}
proposed the Kappa statistic, and its extension $\kappa_m$ \cite{kappa-m},which is a robust estimator that takes into consideration
the probability that a classifier will produce the correct prediction by chance.
The metric has also been expanded to deal with temporal dependence
in the labels of subsequent examples \cite{temporal-dependence}.
More recently, \citet{prequential-auc} proposed an online adaptation
of the area under the ROC curve metric (AUC) \cite{auc}, called
\emph{prequential AUC} which can deal with the class-imbalance
problem and in the presence of potential concept drift, and perform
an evaluation against metrics like the Kappa statistics to show
that each captures a different aspect of the performance.

\section{Online Ensemble Methods}
\label{sec:bg-ol-ensembles}

Due to their approximate nature online learning models often demonstrate
limited accuracy compared to their batch counterparts. Ensemble methods
have been shown to vastly improve the bias \& variance characteristics
of a wide range of models \cite{ensemble-methods-dietrich} and have therefore
also been a focus in the online learning literature.
In this section we will provide an introduction to some of the more established
online ensemble methods that we have also used in our work, along with related
recent work. We refer the interested to the survey by \citet{online-ensembles-survey}
for a more in-depth look at online ensemble methods.

The two main paradigms in ensemble algorithms are bagging \cite{bagging} and boosting \cite{boosting-freund, boosting-schapire}. In this section we will review how each of these algorithms
has been adapted to the online setting. We will focus our descriptions on the first, and more established,
work that deals with both online bagging and boosting proposed by \citet{Oza2001online}.

\subsection{Online Bagging}

The bagging model proposed by \citet{Oza2001online} and further explored in
\cite{online-bagging-experiments} is an intuitive algorithm, listed in Algorithm
\ref{alg:ozabag}. We make use of this algorithm in \uncertaintrees.

\begin{algorithm}
	\small
	\caption{OzaBag(\ensemble, $L_o$, $(x,y)$)}
	\label{alg:ozabag}
	\Initialization{$\lambda^c_t \define \lambda^w_t \define 0 , \quad \forall t \in [1,s]$}
	\Input{\ensemble, the ensemble, a set of $s$ hypotheses $h_t$;
		$L_o$, an online learning algorithm;
		$(x,y)$, a labeled training instance.}
	\Output{hypotheses $h_t$ trained on $(x,y)$.}

	\ForEach(\tcp*[f]{in order $t \in [1,s]$}){$h_t \in \ensemble$}{
		k \define Poisson(1) \;
		Assign weight k to $(x,y)$ \;
		$h_t \define L_o(h_t, (x,y))$
	}
	\Return $\widehat{y}$ \;
\end{algorithm}

The main insight of the algorithm, commonly referred to as \emph{OzaBag}, is that we can approximate the binomial distribution
used to determine whether a sample will be included the bootstrap\todo{First time we mention bootstrap?}
sample using a Poisson distribution, as the distribution of bagging sample sizes, $K$, tends
to a Poisson(1) as the number of samples $N \rightarrow \infty$. To use bagging online then
for each incoming example and
for each member of the ensemble \ensemble, we draw a sample from a Poisson(1). We use to the drawn sample k to modify the weight of the incoming instance, and train the algorithm using the updated weight. \citet{Oza2001online} prove that as the number of samples N grows
to infinity, the distribution of the online training set will converge to that of the batch algorithm.

This strategy has been adapted and extended in multiple online bagging methods. \citet{online-bag-imbalance}
make use of this strategy to deal with the class imbalance problem in online manner. Their strategy
is to adjust the $\lambda$ parameter of the Poisson distribution so that data points with underrepresented
classes have a larger effect on the training. \citet{new-ensemble-methods} provide two alternative
bagging methods with the purpose of dealing with concept drift, one that uses tree of different
sizes, and one that makes use of the ADWIN \cite{adwin} change detection method on top Oza's bagging algorithm. Leveraging Bagging \cite{leveraging-bagging} is an improvement to the previous model,
where the authors increase the $\lambda$ parameter of the Poisson distribution to ``increase the diversity of the weights`` and use the method of \citet{multiclass-codes} to handle multi-class cases
as binary classification using error correcting codes.

\subsection{Online Boosting}

Online boosting has found more widespread use compared to online bagging ensembles,
as it's been used successfully in many computer vision applications, with a focus on
object tracking \cite{online-boost-cv4, online-boost-cv, online-boost-cv3, online-boost-cv2, online-boost-cv5, online-boost-cv-6}.
As a result many different boosting algorithms have been proposed, some, as the ones
mentioned in tracking aiming at a specific application, and others that are more general.

We will focus on the original algorithm proposed by \citeauthor{Oza2001online},
which we also make use of in \boostvht,
and the more recent general online boosting algorithms that improve upon the
theoretical guarantees and performance of the original.

The original online boosting algorithm by \citeauthor{Oza2001online}, referred to as
OzaBoost, is listed
in Algorithm \ref{alg:ozaboost}.
The learning process is similar to that of OzaBag (Algorithm \ref{alg:ozabag}), but now the
algorithm is strictly sequential, and the weight the example takes depends on whether
the previous member of the ensemble was able to correctly classify the example.
Specifically, the algorithm tries to assign half the total weight to the misclassified
example on the next stage, and the other half to the correctly classified ones.
This is done by keeping track of the $\lambda$ parameter sums for the two cases,
correct and incorrect classification, and update the weight of an example
before it passes on to the next member of the ensemble accordingly, increasing
the weight every time is incorrectly classified, and decreasing it every time it
is correctly classified.  Something to note about OzaBoost is that it requires
that we set the number of boosting rounds from the beginning, unlike the
batch AdaBoost algorithm.

Despite its popularity, OzaBoost lacks rigorous theoretical guarantees.
The first attempt to formalize online boosting was made by \citet{online-boosting-theoretical}
They re-visit the assumption made about the performance of the weak
learners, namely that any weak learner will be able to
do better than random guessing, as it is not a realistic assumption
for the online setting where learners are more limited. They also
provide a way to not have to set the number of learners in the ensemble beforehand,
by dynamically assigning voting weights to learners.
However, doing so requires the setting of another parameter $\gamma$,
for which it is hard to determine good values.
Their algorithm, OSBoost,
extends the batch SmoothBoost \cite{smoothboost} algorithm, which
was designed as a boosting algorithm robust to noise, to the online
setting. They use the weighting scheme from that algorithm to assign
larger weights to incorrectly predicted examples, and prove that
their ensemble can use the set of weak learners to achieve a small
error.

The work of \citet{Beygelzimer2015optimal} improves upon OSBoost,
by providing an optimal algorithm in terms of error rate, and also provides
a parameter-free algorithm that does away with the need to set the $\gamma$
parameter of the OSBoost.
The optimal algorithm, Online.BBM, is based on the Boost-by-majority batch
algorithm \cite{batch-bbm} and relaxes the assumptions made by OSBoost,
while the parameter-free algorithm, AdaBoost.OL, makes use of online
loss minimization where the authors choose to minimize logistic loss
in order to avoid large weights which could adversely affect the
error rate of the algorithm.
We should note that all the above algorithms are strictly sequential
and are therefore hard to parallelize.
Our work in \boostvht is able to
make use of any online boosting algorithm to perform
parallel online boosting, while maintaining their guarantees.


\begin{algorithm}
	\small
	\caption{OzaBoost(\ensemble, $L_o$, $(x,y)$)}
	\label{alg:ozaboost}
	\Initialization{$\lambda^c_t \define \lambda^w_t \define 0 , \quad \forall t \in [1,s]$
		\tcp*[f]{cumulative weight of instances with correct and wrong predictions}}
	\Input{\ensemble, the ensemble, a set of $s$ hypotheses $h_t$;
		$L_o$, an online learning algorithm;
		$(x,y)$, a labeled training instance.}
	\Output{prediction $\widehat{y}$.}
	\tcp*[h]{prequential evaluation: first test...} \;
	$\widehat{y} = \argmax_{\dot{y} \in Y} \sum_{t = 1}^{s} \log \left( \frac{1-\epsilon_t}{\epsilon_t} \right) I\left( h_t(x) = \dot{y} \right)$ \;
	\tcp*[h]{...then train} \;
	$\lambda \define 1$\;

	\ForEach(\tcp*[f]{in order $t \in [1,s]$}){$h_t \in \ensemble$}{
		k \define Poisson($\lambda$) \;
		\While(\tcp*[f]{give weight $k$ to the instance}){k > 0}{
			$h_t \define L_o(h_t, (x,y))$ \;
			$k \define k-1$ \;
		}
		\If(\tcp*[f]{correct prediction}){$y = h_t(x)$}{
			$\lambda^c_t \define \lambda^c_t + \lambda$ \;
			$\epsilon_t \define \frac{\lambda^w_t}{\lambda^c_t + \lambda^w_t}$ \;
			$\lambda \define \lambda \left( \frac{1}{2(1 - \epsilon_t)} \right)$ \;
		}\Else(\tcp*[f]{wrong prediction}){
			$\lambda^w_t \define \lambda^w_t + \lambda$ \;
			$\epsilon_t \define \frac{\lambda^w_t}{\lambda^c_t + \lambda^w_t}$ \;
			$\lambda \define \lambda \left( \frac{1}{2 \epsilon_t} \right)$ \;
		}
	}
	\Return $\widehat{y}$ \;
\end{algorithm}

\section{Software}
\label{sec:bg-ol-software}

Compared to the multitude of options available for batch learning, \todo{Cite?} the availability
of open-source software for online learning is relatively limited. In this section we will
mention the most popular libraries for online learning, and point out the ones we have used to develop
our work.

Perhaps the first online learning library released is the Very Fast Machine Learning (VFML) framework which was developed by \citet{vfdt} as part of their work on the VFDT algorithm.
It includes the VFDT and its variations, as well as data pre-processing tools.

One of the most established open-source frameworks for online learning is MOA, which stands
for Massive Online Analysis \cite{bifet2010moa}. MOA includes a collection of
learning algorithms, evaluation methods and metrics, data generators as well as
a graphical interface to perform repeatable experiments. MOA is designed in the
vein of WEKA \cite{weka} and includes interfaces that allow inter-operability
between the algorithms available in WEKA and MOA. Like WEKA, MOA has an extensible
design that allows developers to re-use parts of the existing algorithms to develop
new methods, and easily evaluate new methods using many of the methods mentioned
in Section \ref{sec:bg-ol-evaluation}, by adhering to a simple API. In our work,
we have used MOA to implement the algorithms of \uncertaintrees and as a comparison
in \boostvht. Other single-worker online learning libraries
include LIBOL~\cite{libol} and more recently scikit-multiflow~\cite{sk-multiflow}.

One drawback of these libraries is that they are designed to run on a single machine and therefore
can have issues with massive, distributed streams. Apache SAMOA \cite{samoa} is an effort
to bring the design principles of MOA into the distributed setting, utilizing a platform-independent
design that allows it to run on top of many distributed stream processing engines like
Apache Flink \cite{flink}, Apache Storm~\cite{storm} and Apache Samza~\cite{samza}.
Similarly to MOA, it provides learning algorithms and evaluation methods, and its
design makes it easy to extend with new algorithms.
We have used SAMOA as the distributed engine for \boostvht.

Another library aimed at distributed stream learning is StreamDM \cite{streamdm}.
StreamDM also inherits some of the design aspects of MOA, providing similar interfaces
to access learning algorithms and run evaluations, but unlike SAMOA
is built to run on top of only the Apache Spark~\cite{spark} streaming engine.
While running on top of a distributed stream engine can make the development
of distributed algorithms easier, it can have a negative effect on the
performance of the library, due to the overhead introduced by the
engine. To tackle this is issue, an optimized version of StreamDM
was proposed in \cite{streamdmPP}. An earlier attempt at developing
a distributed streaming learning framework was Jubatus~\cite{jubatus}.

Finally, one of the most successful frameworks that relies on online learning,
albeit to deal with batch problems, is Vowpal Wabbit (VW) \cite{vw}. VW makes
use of parallel and distributed Stochastic Gradient Descent \cite{sgd} as its main
learning algorithm and has been extended to provide a number of online learning models,
including online boosting \cite{Beygelzimer2015optimal}, online Latent Dirichlet Allocation
\cite{ldaOnline}, and contextual bandits \cite{onlineBandits}.
