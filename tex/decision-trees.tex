\chapter{Decision Trees}
\label{ch:bg-decision-trees}

Decision trees is one of the most important and successful algorithms
in machine learning. They have seen widespread adoption in practically
every area of machine learning, from their original domain in classification
and regression, to ranking~\cite{lambdarank}, semi-supervised learning~\cite{semi-super-trees}, survival analysis~\cite{survival-forests},
quantile regression~\cite{meinshausen2006quantile}, and unsupervised learning~\cite{tree-clustering}. In this chapter
we will provide a brief review of the literature on decision trees in
areas that are relevant to our work, starting with a short introduction
in Section \ref{sec:bg-dt-learning-algorithms}, and focusing on ensembles of decision
trees in Section \ref{sec:bg-dt-ensembles}. For a comprehensive review of
decision tree literature we refer the reader to the monograph
by \citet{decision-trees-book} or the extended survey by \citet{tree-survey}.

\section{Learning Algorithms}
\label{sec:bg-dt-learning-algorithms}

The two original tree-learning algorithms, the
Classification and Regression Trees (CART)~\cite{breiman1984cart} algorithm and the ID3
algorithm~\cite{id3}, were developed in parallel. In our description we will
focus on the CART regression algorithm, following the description from \citet{esl}.

Decision trees create recursive binary partitions of the input space,
with axis parallel cuts. Let $R_1, ..., R_M$ denote the $M$ regions created by
the model.
The model predicts the same value $\hat{c}_m$ for all data points
that fall under the same region in the partitioned space. For regression,
using the sum of squared errors as the objective function, the best possible
$\hat{c}_m$ is the average of the label values in that region.
Finding the optimal partitioning for decision trees is known to be
NP-complete \cite{dt-np-complete}. Consequently a common choice
is to use a greedy algorithm to select the splits~\cite{dt-hardness}.
In the regression case, CART achieves that by selecting splits
that minimize the squared error of the created binary split, selecting
for each region the average value of the labels.
In the classification case, the splitting criterion is an information-theoretic
node impurity measure like the Gini index or the cross-entropy, defined as:

\begin{equation}
	\begin{split}
		\text{Gini Index} = \sum_{k=1}^K\hat{p}_{mk}(1 - \hat{p}_{mk}), \\
		\text{Cross-entropy} = -\sum_{k=1}^K\hat{p}_{mk} \log{\hat{p}_{mk}},
	\end{split}
\end{equation}

\noindent
where $k$ is the class, $m$ the node and $\hat{p}_{mk}$ the proportion of
the observations for class $k$ in the node $m$.

While one could keep growing a decision tree until it perfectly fits the data,
that will lead to overfitting and bad generalization. Therefore
it is common to employ some form of \emph{pruning} strategy to limit the
complexity of the model. This can be done by introducing some
form of complexity penalty to the tree, limiting the number of terminal
nodes, or having a minimum number of examples in each terminal node
\cite{breiman1984cart}. This process is also important in limited
memory settings where deleting and merging nodes can lead to models
that take up less space in memory, especially in ensemble approaches
mentioned in the next section. A creative approach to reducing model size are the so called \emph{decision
jungles} ~\cite{decision-jungles} that change the data structure being learned from a tree to a Directed
Acyclical Graph (DAG), allowing
nodes in the DAG to have more than one parent,
providing the same learning capability as a tree, while using fewer nodes.


\section{Ensembles of Decision Trees}
\label{sec:bg-dt-ensembles}

The success and popularity of decision trees comes in large part
from their use in model ensembles.
The two dominant paradigms in ensemble learning are \emph{bagging} \cite{bagging} and
\emph{boosting} \cite{boosting-schapire, boosting-freund-schapire}. Both
methods can be interpreted using a resampling procedure. Bagging is
based on the bootstrap resampling method \cite{bootstrap} and re-uses samples
to train the members of the ensemble, and average their predictions.
% Bagging can be used to reduce both the bias and variance of a learning method.
Boosting methods on the other hand assign increased weights to the
hardest instances in the data. In the ``AdaBoost.M1'' version of the algorithm,
a new weak learner is trained at each iteration, and a weighted sum of
the decisions of each learner is taken to produce the final prediction.
Further work by \citet{gradient-boosting-breiman} established a statistical framework
for boosting and formalized \emph{gradient boosting} as gradient descent in function
space.

Decision trees have been used as parts of either bagging
or boosting ensembles with great success~\cite{hundreds-classifiers, xgboost}. Their characteristics as a fast
non-linear learner
with high flexibility fit well both the bagging and boosting paradigms.
The \emph{random forest} algorithm uses bagging to train an ensemble
of randomized trees and combine their predictions, while gradient boosted trees
uses a combination of trees and gradient boosting to produce
accurate learners. In the following we give a brief overview for each method,
and refer the reader to \cite{random-forest-survey, tree-survey} and \cite{biau-optimization} for recent surveys on
random forests and gradient boosting respectively, or the introductory monograph by \citet{esl}.

\subsection{Random Forests}
\label{sec:bg-dt-random-forests}

Random Forests (RF), originally proposed by \citet{random-forests} based on the work
of \citet{amit-rf}, have been shown to be one of the
most versatile algorithms in machine learning~\cite{hundreds-classifiers}. They are an
ensemble method the combines multiple randomized trees, each trained with a sample
of the original dataset, and averages their predictions to produce the final outcome.
Random forests can deal with classification or regression tasks and provide estimates
of the importance of variables, aiding in their interpretability.

Following the notation in \citet{random-forest-survey}, the algorithm works by growing
$M$ randomized trees, each on a sample $a_n$ from the original dataset, with $p$ features.
The sample
$a_n$ can be taken with or without replacement, although usually it is a boostrap
sample~\cite{bootstrap} with a size equal to the original dataset. In the original
algorithm by \citeauthor{random-forests}, the trees are grown using the CART criteria,
with the difference that they limit the number of features being
considered for the splits, with \citeauthor{random-forests} selecting a either single feature or
$log_2{p} + 1$ features, while \citeauthor{random-forest-survey} recommend using $\lceil p/3 \rceil$;
see \cite{rf-parameters} for an investigation of the RF parameter settings.
For the regression task, the predictions of the forests are calculated
by taking the average prediction of all trees, while for classification
they take a majority vote among all trees.

The random sampling and independent growing of each tree provides two distinct advantages
to the RF algorithm. First, because the trees are grown independently, unlike the boosted trees
discussed in the next section, parallelizing their learning is trivial. In the parallel
setting where we have access to the complete dataset, or if the complete dataset
fits in memory, parallelization is as simple as training a different tree on
each bootstrap sample. The situation changes
however for distributed data because of the nature of the bootstrap: depending
on the sample selection there might be the need for data points to be communicated
over the network, with a resulting high communication cost.
\citet{ensembles-bites} propose instead training the trees independently
and merging their votes, or one can use of the ``bag of little bootstraps''
\cite{bag-of-boostraps} to train the models, to avoid shuffling the complete
dataset. Alternatively, we can create split
histograms on data partitions and communicate those over the cluster to determine the
best split for each leaf, similar to how trees are grown for distributed
gradient boosted tree methods~\cite{xgboost, lightgbm}, which we expand up in
Section \ref{sec:bg-dt-gbts}.

Second, the fact that for each tree some samples from the dataset are not
used to train it, known as \emph{out-of-bag} (OoB) samples, creates opportunities
to use those data points to extract useful information about the performance
of the tree. We can use the OoB samples to adjust the hyper-parameters of the
algorithm without the need to use a validation dataset. We can also use
the out-of-bag sample to estimate \emph{variable importance}.
\citet{random-forests} proposed the Mean Decrease Accuracy measure of
variable importance, where using the OoB instances for a tree, we
randomly permute the values a feature, and make predictions
for the permuted and original data. We take the average
of the difference of the OoB error between the permuted
and original data and use that as an estimate of the importance
of the variable. \citet{random-forest-survey} note that this approach has issues with correlated variables, because the
algorithm tests variables in isolation rather than conditional
to each other.
Finally, \citet{johansson2013conformal} make use of the OoB samples to provide
uncertainty estimates in inductive conformal prediction, without the need for a
validation set, a property we exploit in \uncertaintrees as well.

\subsection{Gradient Boosted Trees}
\label{sec:bg-dt-gbts}

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{boosted-trees}
	\caption{Example of growing a boosted tree ensemble. At each iteration we add a new
	tree, that is trained using the errors (gradients) of the previous ensemble.}
	\label{fig:boosted-trees}
\end{figure}


Gradient boosted trees have risen to be one of the most popular algorithms
in machine learning due to their speed, scalability, and accuracy. As
a result, they are being used in mission-critical production use-cases such as Web search
ranking \cite{mcrank, catboost} and click-through rate prediction \cite{ctr-facebook}.
Efficient
implementations like XGBoost \cite{xgboost} and LightGBM \cite{lightgbm}
are now widely deployed and further emphasis
is being given to efficient training \cite{dimboost} and inference \cite{quickscorer, tree-cache-conscious, tree-runtime-opt}.
In this section we will describe the training and prediction process of GBTs,
with a focus on the scalability aspect of the algorithm. We provide an in-depth
description of the histogram construction algorithm which we have found to be missing
from most relevant literature. We refer the interested reader to a recent overview
of the optimization process of GBTs by \citet{biau-optimization} for details on GBT
training.

\subsubsection*{Model}

The gradient boosted tree model consists of an ensemble of decision trees,
which are trained in an additive manner. At each iteration a new tree is added
to the ensemble, chosen so that it minimizes an objective function. Figure \ref{fig:boosted-trees}
shows the ensemble growing process at a high level. At each iteration we use the
errors (gradients) of the previous step to train a new tree, and add it to the
ensemble. This process is iterated for a pre-determined number of steps.

We use the
regularized objective formulation given
by \citet{xgboost}, and follow their notation throughout our description of the algorithm.
The objective function is shown in Eq. \ref{eq:gbt-objective}:

\begin{equation}
	\begin{split}
	\mathcal{L}(\phi) = \sum_{i} l\left(\hat{y}_{i},y_{i}\right) + \sum_{k} \Omega\left(f_{k}\right) \\
	\text{where } \Omega\left(f_{k}\right) = \gamma T + \frac{1}{2} \lambda \norm{w}^2
	\end{split}
	\label{eq:gbt-objective}
\end{equation}

\noindent
where $l(\cdot, \cdot)$ is a differentiable loss function and $\Omega$ is a regularization term
that penalizes complex models, with $\lambda$ and $\gamma$ being regularization factors. The regularization term takes into consideration
the number of leaves $T$ and the norm of the weights at the leaves $w$. This helps
smooth the weights to avoid overfitting. This objective defined in XGBoost
has then been used in follow-up GBT learning systems like LightGBM and
DimBoost \cite{lightgbm, dimboost}.  A note here is that XGBoost
assumes that the loss function is twice differentiable in order
to calculate the Hessian analytically for each loss function,
in order to reach the optimum quickly.
However, as shown recently in \cite{accelerated-gb, accelerated-gbm} it is possible
to use Nesterov's first-order accelerated gradient descent method
\cite{nesterov-book} to produce gradient boosting models of similar
accuracy, utilizing far fewer trees (iterations), and hence speeding up training
and inference.
In distributed training this has the added benefit of reducing
the amount of data being communicated by half, as we would only need to
communicate first-order gradients and not Hessians, as we explain
in the following section.



\subsubsection*{Training}

\begin{table}
	\centering
	\begin{tabular}{cccc|c}
		\toprule
		Index & Feature 1 & Feature 2 & Feature 3 & Gradient \\
		\midrule
		\rowcolor{CBOne}
		1 & 13 & 0 & 488 & 1.5 \\
		\rowcolor{CBTwo}
		2 & 7 & 1 & 667 & 2.5 \\
		\rowcolor{CBThree}
		3 & 3.5 & 0 & 122 & 2 \\
		\rowcolor{CBFour}
		4 & 1.6 & 2 & 366 & 2.5 \\
		\bottomrule
	\end{tabular}
	\caption{Example dataset. Each color-coded data point has its feature values, along
	with a gradient value, for a squared error objective function, this would be the ensemble's
	residual for the data point.}
	\label{tab:example-data}
\end{table}


The training process for a single gradient boosted tree can be roughly divided into three stages:
First, we use the existing ensemble to make predictions for every data point in the training
set. Second, we use the approximated objective function to determine the first and second order
gradients for each data point. Finally, we use the calculated gradients to greedily determine
the optimal split for every leaf in the tree. When we mention gradients here we are referring
to both the first order and second order gradients of the loss function for $x_i$, denoted as $g_i$ and $h_i$
respectively.

We will focus on the third part of the training stage in our description, as that entails the most
computationally intensive part of GBT learning, the construction of the \emph{gradient histograms.}
Throughout our examples we'll be using the dataset from Table \ref{tab:example-data},
which we have color-coded to correspond to the figures used later.

As a tree grows, we partition the complete training set into smaller parts, with each data
point ending up in one leaf as we drop it down the tree.
In order to determine the best split for every leaf, we need to iterate through the data
partition of each leaf and create a gradient histogram for each feature for that leaf.

In its simplest form, a gradient histogram gives us the sum of gradients at a leaf,
given a feature value, that is, $\sum_{j}(G_i | x_i(f) = f_j)$, where $x_i$ is a data point
that belongs to the current leaf,  $G_i$ is the gradient value of $x_i$ and $j$ is an
indicator over the unique values of the feature $f$. In other words, it gives us the
sum of the gradients of all data points in the leaf for a specific value of a given
feature $f$.

Enumerating every gradient-feature value combination requires that we calculate
a gradient sum for each unique value
of a feature. This can quickly become computationally impractical for real-valued features, as we might have millions or billions of unique feature values.
Systems like XGBoost and LightGBM instead use \emph{quantized histograms},
where we accumulate gradients for ranges of values in histogram buckets for every feature.

To determine the bucket ranges we
can use \emph{quantile sketches} \cite{greenwald2016quantiles} that approximate
the Cumulative Distribution Function (CDF) of each feature. We select a number of
equi-weight buckets, that is, the ranges are selected such that every bucket
has approximately the same weight-sum of data points. In the simplest case
all instances have the same weight, and regular quantile sketch algorithms
can be used, otherwise specialized, weighted sketches like the ones used by XGBoost
have to be employed.
The number of buckets to use is a
parameter of the quantile sketch algorithm and depends on the type of sketch.
There exist sketches where we select the number of buckets that
will be used to represent the CDF, like the ones proposed by \citet{BenHaim2010parallel}.
Alternatively, the number of buckets is derived from the approximation error we choose for the sketches, for example
for the sketches used by XGBoost or by the state-of-the-art KLL sketch \cite{karnin2016kll}
which we used in our own work in Papers \uncertaintreesNum and \blockgbtNum.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{feature-one-grad}
	\caption{Gradient histogram for Feature 1 of the Table \ref{tab:example-data} data.
	The first bucket is the sum of the data points with index 3 and 4, since both their Feature 1
	values fall in the $[0, 5)$ bucket.}
	\label{fig:gradient-feature-one}
\end{figure}


The selection of bucket ranges can be done either at the beginning of training, taking the
overall feature CDF, or at every leaf taking the CDF of each feature using only the
partition for that leaf. \citet{xgboost} show that by selecting the buckets for every
leaf separately, we can achieve the same level of accuracy, while using histograms with a higher approximation error, and hence reducing the memory footprint of the algorithm.


Once the bucket ranges have been determined for each feature, we use them to create
the gradient histograms. After the prediction step, we have a gradient value for each data point, so
we go through each feature and add the data point's gradient value to the gradient
histogram bucket
that corresponds to the feature value.
In our example data in Table \ref{tab:example-data}, for Feature 1 we use the
buckets $[0, 5), (5, 12], (12, 20)$.
Then, for the data point $x_1$ with a feature value $f_1 : 13$, we would add its
gradient value to the
the bucket $(12, 20]$ of the gradient histogram for $f_1$, and do this for
every data point, adding the point's gradient to the corresponding bucket given
the feature value. The gradient histogram for Feature 1 is shown in Figure \ref{fig:gradient-feature-one}. In the Figure we use color to identify the
contribution of each data point to each bucket.
When we are done iterating, each feature will have a corresponding gradient histogram,
whose sum is equal to the sum of gradients for the leaf.
For the data of Table \ref{tab:example-data}, the complete gradient histograms
are shown in Figure \ref{fig:gradient-all-features}.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{all-features-grad-custom}
	\caption{Gradient histograms for all features of the Table \ref{tab:example-data} data.
		}
	\label{fig:gradient-all-features}
\end{figure}

Once we have all the gradient histograms, we can use them to determine the optimal split point for the
leaf, by calculating the potential gain of each feature and split point combination using the following equation~\cite{xgboost}:

\begin{equation}
	\mathcal{G}_{\text{split}} = \frac{1}{2} \left[\frac{\left(\sum_{i \in I_{L}} g_{i} \right)^{2}} {\sum_{i \in I_{L}} h_{i}+ \lambda} + \frac{\left(\sum_{i \in I_{R}} g_{i} \right)^{2}}{\sum_{i \in I_{R}} h_{i} + \lambda} - \frac{\left(\sum_{i \in I}g_{i}\right)^{2}}{\sum_{i \in I}h_{i}+\lambda}\right]-\gamma
\end{equation}

\noindent
where $I_L, I_R$ determine the data points that end up to the left and right side of the split
respectively, $I$ is the complete set of data for the leaf partition, and $\gamma$, $\lambda$ are
regularization parameters. At this point, to determine
the best split we need to simply iterate through all the feature-splitpoint combinations
and rank every candidate according to their loss reduction (or \textit{gain}) and select the
best one. In our example we have three buckets, and hence two possible split points for each feature. The result for our example data is shown in Table \ref{tab:gains}, where
we have simplified the problem to only include the first order gradients. From that example
we can see that the best feature-split combination would be choosing the first split
point for the first feature (i.e. Feature 1 $< 5$) as the split condition.

\begin{table}
	\centering
	\begin{tabular}{ccc}
		\toprule
		& Splitpoint 1 & Splitpoint 2 \\
		\midrule
		Feature 1 & \textbf{36} & 21 \\
		Feature 2 & 35 & 30 \\
		Feature 3 & 26 & 30 \\
		\bottomrule
	\end{tabular}
	\caption{Potential gains for each possible split point, given
	the gradient histograms of Figure \ref{fig:gradient-all-features}.}
	\label{tab:gains}
\end{table}

In the case of millions of features and a large bucket count $B$ this can also be
computationally heavy operation, as we need to evaluate $|F| \times B$ split candidates.
To mitigate this, we can apply efficient boosting approaches that try
to reduce the number of features being examined. Examples include LazyBoost \cite{lazyboost}
that uniformly subsamples the set of features,
bandit methods \cite{bandits-boosting} that use information from previous iterations, or other adaptive
methods like Laminating \cite{laminating} that determines the best feature in multiple rounds, by incrementally
halving the number of features being considered and doubling the number of examples being
included in the gradient calculations.

